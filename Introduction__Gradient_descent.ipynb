{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MithunSR/Gradient_Descent_Tutorial/blob/main/Introduction__Gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MytjHxDmBPTi"
      },
      "source": [
        "\n",
        "\n",
        "# Introduction to Gradient Descent\n",
        "\n",
        "The Idea Behind Gradient Descent \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TnJKK3gBPT0"
      },
      "source": [
        "![](https://github.com/chengjun/mybook/blob/main/img/stats/gradient_descent.gif?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaRQoDa8BPT3"
      },
      "source": [
        "![](https://github.com/chengjun/mybook/blob/main/img/stats/gradient.png?raw=1)\n",
        "\n",
        "**Gradient is the vector of partial derivatives**\n",
        "\n",
        "One approach to maximizing a function is to\n",
        "- pick a random starting point, \n",
        "- compute the gradient, \n",
        "- take a small step in the direction of the gradient, and \n",
        "- repeat with a new staring point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOeYAAm1BPT5"
      },
      "source": [
        "\n",
        "![](https://github.com/chengjun/mybook/blob/main/images/gradient.gif?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXvtBcdsBPT6"
      },
      "source": [
        "![](https://github.com/chengjun/mybook/blob/main/img/stats/gd.webp?raw=1)\n",
        "\n",
        "Let's represent parameters as $\\Theta$, learning rate as $\\alpha$, and gradient as $\\bigtriangledown J(\\Theta)$, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEDXW8erBPT7"
      },
      "source": [
        "To the find the best model is an optimization problem\n",
        "- “minimizes the error of the model” \n",
        "- “maximizes the likelihood of the data.” "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwCqiaIIBPT8"
      },
      "source": [
        "We’ll frequently need to maximize (or minimize)  functions. \n",
        "- to find the input vector v that produces the largest (or smallest) possible value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-21T13:02:56.099816Z",
          "start_time": "2021-11-21T13:02:56.047568Z"
        },
        "id": "yZqEMMDvBPT8"
      },
      "source": [
        "### Mannual Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T08:09:56.505760Z",
          "start_time": "2021-11-22T08:09:56.316148Z"
        },
        "id": "L2G9JRD7BPT-",
        "outputId": "42263373-3061-437f-d2ce-794e7bc7820b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "plt.plot(x_data, y_data, 'ro')\n",
        "plt.ylabel('y')\n",
        "plt.xlabel('x');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-21T13:02:56.099816Z",
          "start_time": "2021-11-21T13:02:56.047568Z"
        },
        "id": "o-NxJ28sBPUB"
      },
      "outputs": [],
      "source": [
        "# our model for the forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "# Loss function\n",
        "def loss(y_pred, y_val):\n",
        "    return (y_pred - y_val) ** 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T08:11:05.957597Z",
          "start_time": "2021-11-22T08:11:05.952545Z"
        },
        "id": "suQqxn5iBPUD"
      },
      "outputs": [],
      "source": [
        "# List of weights/Mean square Error (Mse) for each input\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "    # Print the weights and initialize the lost\n",
        "    #print(\"w=\", w)\n",
        "    l_sum = 0\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # For each input and output, calculate y_hat\n",
        "        # Compute the total loss and add to the total error\n",
        "        y_pred = forward(x_val)\n",
        "        l = loss(y_pred, y_val)\n",
        "        l_sum += l\n",
        "        #print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "    # Now compute the Mean squared error (mse) of each\n",
        "    # Aggregate the weight/mse from this run\n",
        "    #print(\"MSE=\", l_sum / 3)\n",
        "    w_list.append(w)\n",
        "    mse_list.append(l_sum / 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T08:11:13.631135Z",
          "start_time": "2021-11-22T08:11:13.442089Z"
        },
        "id": "C7brU73rBPUE",
        "outputId": "411c0b0b-344a-4c39-ddf7-5af90f91b83f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot it all\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-22T08:11:36.517072Z",
          "start_time": "2021-11-22T08:11:36.460683Z"
        },
        "id": "qxwnP5kNBPUF",
        "outputId": "891d4401-8562-4f77-c00e-f69ac11cce6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 w= 3.48 loss= 19.68\r",
            "Epoch: 1 w= 3.09 loss= 10.76\r",
            "Epoch: 2 w= 2.81 loss= 5.88\r",
            "Epoch: 3 w= 2.6 loss= 3.21\r",
            "Epoch: 4 w= 2.44 loss= 1.76\r",
            "Epoch: 5 w= 2.33 loss= 0.96\r",
            "Epoch: 6 w= 2.24 loss= 0.52\r",
            "Epoch: 7 w= 2.18 loss= 0.29\r",
            "Epoch: 8 w= 2.13 loss= 0.16\r",
            "Epoch: 9 w= 2.1 loss= 0.09\r",
            "Epoch: 10 w= 2.07 loss= 0.05\r",
            "Epoch: 11 w= 2.05 loss= 0.03\r",
            "Epoch: 12 w= 2.04 loss= 0.01\r",
            "Epoch: 13 w= 2.03 loss= 0.01\r",
            "Epoch: 14 w= 2.02 loss= 0.0\r",
            "Epoch: 15 w= 2.02 loss= 0.0\r",
            "Epoch: 16 w= 2.01 loss= 0.0\r",
            "Epoch: 17 w= 2.01 loss= 0.0\r",
            "Epoch: 18 w= 2.01 loss= 0.0\r",
            "Epoch: 19 w= 2.0 loss= 0.0\r",
            "Epoch: 20 w= 2.0 loss= 0.0\r",
            "Epoch: 21 w= 2.0 loss= 0.0\r",
            "Epoch: 22 w= 2.0 loss= 0.0\r",
            "Epoch: 23 w= 2.0 loss= 0.0\r",
            "Epoch: 24 w= 2.0 loss= 0.0\r",
            "Epoch: 25 w= 2.0 loss= 0.0\r",
            "Epoch: 26 w= 2.0 loss= 0.0\r",
            "Epoch: 27 w= 2.0 loss= 0.0\r",
            "Epoch: 28 w= 2.0 loss= 0.0\r",
            "Epoch: 29 w= 2.0 loss= 0.0\r",
            "Epoch: 30 w= 2.0 loss= 0.0\r",
            "Epoch: 31 w= 2.0 loss= 0.0\r",
            "Epoch: 32 w= 2.0 loss= 0.0\r",
            "Epoch: 33 w= 2.0 loss= 0.0\r",
            "Epoch: 34 w= 2.0 loss= 0.0\r",
            "Epoch: 35 w= 2.0 loss= 0.0\r",
            "Epoch: 36 w= 2.0 loss= 0.0\r",
            "Epoch: 37 w= 2.0 loss= 0.0\r",
            "Epoch: 38 w= 2.0 loss= 0.0\r",
            "Epoch: 39 w= 2.0 loss= 0.0\r",
            "Epoch: 40 w= 2.0 loss= 0.0\r",
            "Epoch: 41 w= 2.0 loss= 0.0\r",
            "Epoch: 42 w= 2.0 loss= 0.0\r",
            "Epoch: 43 w= 2.0 loss= 0.0\r",
            "Epoch: 44 w= 2.0 loss= 0.0\r",
            "Epoch: 45 w= 2.0 loss= 0.0\r",
            "Epoch: 46 w= 2.0 loss= 0.0\r",
            "Epoch: 47 w= 2.0 loss= 0.0\r",
            "Epoch: 48 w= 2.0 loss= 0.0\r",
            "Epoch: 49 w= 2.0 loss= 0.0\r",
            "Epoch: 50 w= 2.0 loss= 0.0\r",
            "Epoch: 51 w= 2.0 loss= 0.0\r",
            "Epoch: 52 w= 2.0 loss= 0.0\r",
            "Epoch: 53 w= 2.0 loss= 0.0\r",
            "Epoch: 54 w= 2.0 loss= 0.0\r",
            "Epoch: 55 w= 2.0 loss= 0.0\r",
            "Epoch: 56 w= 2.0 loss= 0.0\r",
            "Epoch: 57 w= 2.0 loss= 0.0\r",
            "Epoch: 58 w= 2.0 loss= 0.0\r",
            "Epoch: 59 w= 2.0 loss= 0.0\r",
            "Epoch: 60 w= 2.0 loss= 0.0\r",
            "Epoch: 61 w= 2.0 loss= 0.0\r",
            "Epoch: 62 w= 2.0 loss= 0.0\r",
            "Epoch: 63 w= 2.0 loss= 0.0\r",
            "Epoch: 64 w= 2.0 loss= 0.0\r",
            "Epoch: 65 w= 2.0 loss= 0.0\r",
            "Epoch: 66 w= 2.0 loss= 0.0\r",
            "Epoch: 67 w= 2.0 loss= 0.0\r",
            "Epoch: 68 w= 2.0 loss= 0.0\r",
            "Epoch: 69 w= 2.0 loss= 0.0\r",
            "Epoch: 70 w= 2.0 loss= 0.0\r",
            "Epoch: 71 w= 2.0 loss= 0.0\r",
            "Epoch: 72 w= 2.0 loss= 0.0\r",
            "Epoch: 73 w= 2.0 loss= 0.0\r",
            "Epoch: 74 w= 2.0 loss= 0.0\r",
            "Epoch: 75 w= 2.0 loss= 0.0\r",
            "Epoch: 76 w= 2.0 loss= 0.0\r",
            "Epoch: 77 w= 2.0 loss= 0.0\r",
            "Epoch: 78 w= 2.0 loss= 0.0\r",
            "Epoch: 79 w= 2.0 loss= 0.0\r",
            "Epoch: 80 w= 2.0 loss= 0.0\r",
            "Epoch: 81 w= 2.0 loss= 0.0\r",
            "Epoch: 82 w= 2.0 loss= 0.0\r",
            "Epoch: 83 w= 2.0 loss= 0.0\r",
            "Epoch: 84 w= 2.0 loss= 0.0\r",
            "Epoch: 85 w= 2.0 loss= 0.0\r",
            "Epoch: 86 w= 2.0 loss= 0.0\r",
            "Epoch: 87 w= 2.0 loss= 0.0\r",
            "Epoch: 88 w= 2.0 loss= 0.0\r",
            "Epoch: 89 w= 2.0 loss= 0.0\r",
            "Epoch: 90 w= 2.0 loss= 0.0\r",
            "Epoch: 91 w= 2.0 loss= 0.0\r",
            "Epoch: 92 w= 2.0 loss= 0.0\r",
            "Epoch: 93 w= 2.0 loss= 0.0\r",
            "Epoch: 94 w= 2.0 loss= 0.0\r",
            "Epoch: 95 w= 2.0 loss= 0.0\r",
            "Epoch: 96 w= 2.0 loss= 0.0\r",
            "Epoch: 97 w= 2.0 loss= 0.0\r",
            "Epoch: 98 w= 2.0 loss= 0.0\r",
            "Epoch: 99 w= 2.0 loss= 0.0\r"
          ]
        }
      ],
      "source": [
        "# compute gradient\n",
        "def gradient(x, y):  # d_loss/d_w\n",
        "    return 2 * x * (x * w - y)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # Compute derivative w.r.t to the learned weights\n",
        "        # Update the weights\n",
        "        # Compute the loss and print progress\n",
        "        grad = gradient(x_val, y_val)\n",
        "        w = w - 0.01 * grad\n",
        "        #print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
        "        y_pred = forward(x_val)\n",
        "        l = loss(y_pred, y_val)\n",
        "    print(\"Epoch:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2),  end='\\r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOa9buvSBPUG"
      },
      "source": [
        "## Mathematics behind Gradient Descent\n",
        "\n",
        "A simple mathematical intuition behind one of the commonly used optimisation algorithms in Machine Learning.\n",
        "\n",
        "https://www.douban.com/note/713353797/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6DZGQOVBPUG"
      },
      "source": [
        "The cost or loss function:\n",
        "\n",
        "$$Cost = \\frac{1}{N} \\sum_{i = 1}^N (Y' -Y)^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF_b8hEtBPUH"
      },
      "source": [
        "![](https://github.com/chengjun/mybook/blob/main/img/stats/x2.webp?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov6BXi81BPUH"
      },
      "source": [
        "Parameters with small changes:\n",
        "$$ m_1 = m_0 - \\delta m,  b_1 = b_0 - \\delta b$$\n",
        "\n",
        "The cost function J is a function of m and b:\n",
        "\n",
        "$$J_{m, b} = \\frac{1}{N} \\sum_{i = 1}^N (Y' -Y)^2 = \\frac{1}{N} \\sum_{i = 1}^N Error_i^2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcxrWdS2BPUH"
      },
      "source": [
        "$$\\frac{\\partial J}{\\partial m} = 2 Error \\frac{\\partial}{\\partial m}Error$$\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial b} = 2 Error \\frac{\\partial}{\\partial b}Error$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCzWEDDwBPUH"
      },
      "source": [
        "Let's fit the data with linear regression:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial m}Error = \\frac{\\partial}{\\partial m}(Y' - Y) = \\frac{\\partial}{\\partial m}(mX + b - Y)$$\n",
        "\n",
        "Since $X, b, Y$ are constant:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial m}Error = X$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLWJFM3tBPUI"
      },
      "source": [
        "$$\\frac{\\partial}{\\partial b}Error = \\frac{\\partial}{\\partial b}(Y' - Y) = \\frac{\\partial}{\\partial b}(mX + b - Y)$$\n",
        "\n",
        "Since $X, m, Y$ are constant:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial m}Error = 1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAp0A8szBPUI"
      },
      "source": [
        "Thus:\n",
        "    \n",
        "$$\\frac{\\partial J}{\\partial m} = 2 * Error * X$$\n",
        "$$\\frac{\\partial J}{\\partial b} = 2 * Error$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfjtV_31BPUI"
      },
      "source": [
        "Let's get rid of the constant 2 and multiplying the learning rate  $\\alpha$, who determines how large a step to take:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial m} = Error * X *  \\alpha$$\n",
        "$$\\frac{\\partial J}{\\partial b} = Error * \\alpha$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B66sBx2ABPUI"
      },
      "source": [
        "Since $ m_1 = m_0 - \\delta m,  b_1 = b_0 - \\delta b$:\n",
        "\n",
        "$$ m_1 = m_0 -  Error * X * \\alpha$$\n",
        "\n",
        "$$b_1 = b_0 - Error * \\alpha$$\n",
        "\n",
        "**Notice** that the slope b can be viewed as the beta value for X = 1. Thus, the above two equations are in essence the same.\n",
        "\n",
        "Let's represent parameters as $\\Theta$, learning rate as $\\alpha$, and gradient as $\\bigtriangledown J(\\Theta)$, we have:\n",
        "\n",
        "\n",
        "$$\\Theta_1 = \\Theta_0 - \\alpha \\bigtriangledown J(\\Theta)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEDwaxvwBPUJ"
      },
      "source": [
        "\n",
        "\n",
        "<img src='https://github.com/chengjun/mybook/blob/main/img/stats/gd.webp?raw=1' width = '800' align = 'center'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGmaNGA6BPUJ"
      },
      "source": [
        "Hence,to solve for the gradient, we iterate through our data points using our new $m$ and $b$ values and compute the partial derivatives. \n",
        "\n",
        "This new gradient tells us \n",
        "- the slope of our cost function at our current position  \n",
        "- the direction we should move to update our parameters. \n",
        "\n",
        "- The size of our update is controlled by the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:16:42.584919Z",
          "start_time": "2019-04-07T16:16:42.573596Z"
        },
        "id": "RDhkhVUFBPUJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Size of the points dataset.\n",
        "m = 20\n",
        "# Points x-coordinate and dummy value (x0, x1).\n",
        "X0 = np.ones((m, 1))\n",
        "X1 = np.arange(1, m+1).reshape(m, 1)\n",
        "X = np.hstack((X0, X1))\n",
        "# Points y-coordinate\n",
        "y = np.array([3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,\n",
        "    11, 13, 13, 16, 17, 18, 17, 19, 21]).reshape(m, 1)\n",
        "\n",
        "# The Learning Rate alpha.\n",
        "alpha = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:17:04.134904Z",
          "start_time": "2019-04-07T16:17:04.108505Z"
        },
        "id": "JZtL6mWQBPUK"
      },
      "outputs": [],
      "source": [
        "def error_function(theta, X, y):\n",
        "    '''Error function J definition.'''\n",
        "    diff = np.dot(X, theta) - y\n",
        "    return (1./2*m) * np.dot(np.transpose(diff), diff)\n",
        "\n",
        "def gradient_function(theta, X, y):\n",
        "    '''Gradient of the function J definition.'''\n",
        "    diff = np.dot(X, theta) - y\n",
        "    return (1./m) * np.dot(np.transpose(X), diff)\n",
        "\n",
        "def gradient_descent(X, y, alpha):\n",
        "    '''Perform gradient descent.'''\n",
        "    theta = np.array([1, 1]).reshape(2, 1)\n",
        "    gradient = gradient_function(theta, X, y)\n",
        "    while not np.all(np.absolute(gradient) <= 1e-5):\n",
        "        theta = theta - alpha * gradient\n",
        "        gradient = gradient_function(theta, X, y)\n",
        "    return theta\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:19:07.236028Z",
          "start_time": "2019-04-07T16:19:07.171443Z"
        },
        "id": "PTnv6td8BPUK",
        "outputId": "4daa673e-75f5-431a-f0fe-96ade55c0fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal parameters Theta: 0.5158328581734093 0.9699216324486175\n",
            "Error function: 405.98496249324046\n"
          ]
        }
      ],
      "source": [
        "optimal = gradient_descent(X, y, alpha)\n",
        "print('Optimal parameters Theta:', optimal[0][0], optimal[1][0])\n",
        "print('Error function:', error_function(optimal, X, y)[0,0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2-TMOQ4BPUL"
      },
      "source": [
        "## Estimating the Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSGXIvn1BPUL"
      },
      "source": [
        "If f is a function of one variable, its derivative at a point x measures how f(x) changes when we make a very small change to x. \n",
        "\n",
        "> It is defined as the limit of the difference quotie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:08:29.774271Z",
          "start_time": "2019-04-07T16:08:29.771043Z"
        },
        "id": "tsxQonbzBPUL"
      },
      "outputs": [],
      "source": [
        "def difference_quotient(f, x, h):\n",
        "    return (f(x + h) - f(x)) / h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhasS1sKBPUM"
      },
      "source": [
        "For many functions it’s easy to exactly calculate derivatives. \n",
        "\n",
        "For example, the square function:\n",
        "\n",
        "        def square(x): \n",
        "            return x * x\n",
        "\n",
        "has the derivative:\n",
        "    \n",
        "        def derivative(x): \n",
        "            return 2 * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:08:30.714322Z",
          "start_time": "2019-04-07T16:08:30.709209Z"
        },
        "id": "0d7g5-NiBPUM"
      },
      "outputs": [],
      "source": [
        "def square(x):\n",
        "    return x * x\n",
        "\n",
        "def derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:09:28.375610Z",
          "start_time": "2019-04-07T16:09:28.372132Z"
        },
        "id": "d32fAbyiBPUM"
      },
      "outputs": [],
      "source": [
        "def sum_of_squares(v):\n",
        "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
        "    return sum(v_i ** 2 for v_i in v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:11:00.628162Z",
          "start_time": "2019-04-07T16:11:00.425853Z"
        },
        "id": "nzT25OQyBPUN",
        "outputId": "02780a8d-de3a-450a-9b84-b498ed7d19cd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot to show they're basically the same\n",
        "import matplotlib.pyplot as plt\n",
        "x = range(-10,10)\n",
        "plt.plot(x, list(map(derivative, x)), 'rx')           # red  x\n",
        "plt.plot(x, list(map(derivative_estimate, x)), 'b+')  # blue +\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv0D6yEvBPUO"
      },
      "source": [
        "When f is a function of many variables, it has multiple partial derivatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:08:32.859665Z",
          "start_time": "2019-04-07T16:08:32.850606Z"
        },
        "id": "8iyD1QAPBPUb"
      },
      "outputs": [],
      "source": [
        "def partial_difference_quotient(f, v, i, h):\n",
        "    # add h to just the i-th element of v\n",
        "    w = [v_j + (h if j == i else 0)\n",
        "         for j, v_j in enumerate(v)]\n",
        "    return (f(w) - f(v)) / h\n",
        "\n",
        "def estimate_gradient(f, v, h=0.00001):\n",
        "    return [partial_difference_quotient(f, v, i, h)\n",
        "            for i, _ in enumerate(v)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ9ED3cyBPUc"
      },
      "source": [
        "### Using the Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:08:34.771460Z",
          "start_time": "2019-04-07T16:08:34.766201Z"
        },
        "id": "emeLTN8rBPUc"
      },
      "outputs": [],
      "source": [
        "def step(v, direction, step_size):\n",
        "    \"\"\"move step_size in the direction from v\"\"\"\n",
        "    return [v_i + step_size * direction_i\n",
        "            for v_i, direction_i in zip(v, direction)]\n",
        "\n",
        "def sum_of_squares_gradient(v):\n",
        "    return [2 * v_i for v_i in v]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:30:36.585523Z",
          "start_time": "2019-04-07T16:30:36.581608Z"
        },
        "id": "3mTqXQ2nBPUd"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from linear_algebra import distance, vector_subtract, scalar_multiply\n",
        "from functools import reduce\n",
        "import math, random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:34:01.449426Z",
          "start_time": "2019-04-07T16:34:01.419878Z"
        },
        "id": "iLjbuvo9BPUd",
        "outputId": "20810ec1-5b84-4b32-a04a-73bd6c8ec324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using the gradient\n",
            "[-4, 10, 6]\n",
            "[-4, 10, 6] 152\n",
            "[-1.4566787203484681, 3.641696800871171, 2.1850180805227026] 20.15817249600249\n",
            "[-0.5304782235790126, 1.3261955589475318, 0.7957173353685193] 2.6733678840696777\n",
            "[-0.19318408497395115, 0.482960212434878, 0.28977612746092685] 0.35454086152861664\n",
            "[-0.07035178642288623, 0.17587946605721566, 0.10552767963432938] 0.04701905160246833\n",
            "[-0.025619987555179666, 0.0640499688879492, 0.03842998133276951] 0.006235645742111834\n",
            "[-0.0093300226718057, 0.023325056679514268, 0.013995034007708552] 0.0008269685690358806\n",
            "[-0.0033977113715970304, 0.008494278428992584, 0.005096567057395551] 0.00010967220436445803\n",
            "[-0.0012373434632228497, 0.003093358658057131, 0.0018560151948342769] 1.4544679036813049e-05\n",
            "[-0.0004506029731597509, 0.0011265074328993788, 0.0006759044597396269] 1.9289088744938724e-06\n",
            "[-0.00016409594058189027, 0.00041023985145472635, 0.00024614391087283554] 2.558110382968256e-07\n",
            "[-5.975876618530154e-05, 0.00014939691546325416, 8.963814927795238e-05] 3.3925546291900725e-08\n",
            "[-2.1762330764102098e-05, 5.440582691025532e-05, 3.264349614615315e-05] 4.499190882718763e-09\n",
            "[-7.925181032313087e-06, 1.9812952580782747e-05, 1.1887771548469638e-05] 5.96680696751885e-10\n",
            "[-2.8861106411699463e-06, 7.215276602924873e-06, 4.32916596175492e-06] 7.913152901420692e-11\n",
            "minimum v [-1.6064572436336709e-06, 4.0161431090841815e-06, 2.409685865450507e-06]\n",
            "minimum value 2.4516696318419405e-11\n"
          ]
        }
      ],
      "source": [
        "print(\"using the gradient\")\n",
        "\n",
        "# generate 3 numbers \n",
        "v = [random.randint(-10,10) for i in range(3)]\n",
        "print(v)\n",
        "tolerance = 0.0000001\n",
        "\n",
        "n = 0\n",
        "while True:\n",
        "    gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
        "    if n%50 ==0:\n",
        "        print(v, sum_of_squares(v))\n",
        "    next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
        "    if distance(next_v, v) < tolerance:     # stop if we're converging\n",
        "        break\n",
        "    v = next_v                              # continue if we're not\n",
        "    n += 1\n",
        "\n",
        "print(\"minimum v\", v)\n",
        "print(\"minimum value\", sum_of_squares(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEOkSN8_BPUd"
      },
      "source": [
        "### Choosing the Right Step Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7VcTizyBPUe"
      },
      "source": [
        "Although the rationale for moving against the gradient is clear, \n",
        "- how far to move is not. \n",
        "    - Indeed, choosing the right step size is more of an art than a science."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gDPBmaqBPUe"
      },
      "source": [
        "Methods:\n",
        "1. Using a fixed step size\n",
        "1. Gradually shrinking the step size over time\n",
        "1. At each step, choosing the step size that minimizes the value of the objective function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:13:14.748507Z",
          "start_time": "2019-04-07T16:13:14.745314Z"
        },
        "id": "SgHoGC5dBPUj"
      },
      "outputs": [],
      "source": [
        "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_f6iv6cBPUk"
      },
      "source": [
        "It is possible that certain step sizes will result in invalid inputs for our function. \n",
        "\n",
        "So we’ll need to create a “safe apply” function\n",
        "- returns infinity for invalid inputs:\n",
        "    - which should never be the minimum of anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:13:15.408907Z",
          "start_time": "2019-04-07T16:13:15.403349Z"
        },
        "id": "e-9MZRQ5BPUk"
      },
      "outputs": [],
      "source": [
        "def safe(f):\n",
        "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
        "    def safe_f(*args, **kwargs):\n",
        "        try:\n",
        "            return f(*args, **kwargs)\n",
        "        except:\n",
        "            return float('inf')  # this means \"infinity\" in Python\n",
        "    return safe_f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC_m80EHBPUk"
      },
      "source": [
        "### Putting It All Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZnM3XTvBPUl"
      },
      "source": [
        "- **target_fn** that we want to minimize\n",
        "- **gradient_fn**. \n",
        "\n",
        "For example, the target_fn could represent the errors in a model as a function of its parameters, \n",
        "\n",
        "To choose a starting value for the parameters `theta_0`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:13:17.051767Z",
          "start_time": "2019-04-07T16:13:17.028782Z"
        },
        "code_folding": [],
        "id": "I6QeduieBPUl"
      },
      "outputs": [],
      "source": [
        "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
        "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
        "\n",
        "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
        "\n",
        "    theta = theta_0                           # set theta to initial value\n",
        "    target_fn = safe(target_fn)               # safe version of target_fn\n",
        "    value = target_fn(theta)                  # value we're minimizing\n",
        "\n",
        "    while True:\n",
        "        gradient = gradient_fn(theta)\n",
        "        next_thetas = [step(theta, gradient, -step_size)\n",
        "                       for step_size in step_sizes]\n",
        "\n",
        "        # choose the one that minimizes the error function\n",
        "        next_theta = min(next_thetas, key=target_fn)\n",
        "        next_value = target_fn(next_theta)\n",
        "\n",
        "        # stop if we're \"converging\"\n",
        "        if abs(value - next_value) < tolerance:\n",
        "            return theta\n",
        "        else:\n",
        "            theta, value = next_theta, next_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-07T16:13:17.853373Z",
          "start_time": "2019-04-07T16:13:17.845012Z"
        },
        "id": "DV5csr2MBPUl",
        "outputId": "357729f6-617f-41e6-cf4e-6ea6a6610a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "minimum v [0.0009304595970494407, -0.001196305196206424, -0.00026584559915698326]\n",
            "minimum value 2.367575066803034e-06\n"
          ]
        }
      ],
      "source": [
        "# minimize_batch\"\n",
        "v = [random.randint(-10,10) for i in range(3)]\n",
        "v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
        "print(\"minimum v\", v)\n",
        "print(\"minimum value\", sum_of_squares(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waAD9U0mBPUm"
      },
      "source": [
        "Sometimes we’ll instead want to maximize a function, which we can do by minimizing its negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-10-27T07:13:08.983326Z",
          "start_time": "2018-10-27T07:13:08.974094Z"
        },
        "id": "dVvKNo5zBPUm"
      },
      "outputs": [],
      "source": [
        "def negate(f):\n",
        "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
        "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
        "\n",
        "def negate_all(f):\n",
        "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
        "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
        "\n",
        "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
        "    return minimize_batch(negate(target_fn),\n",
        "                          negate_all(gradient_fn),\n",
        "                          theta_0,\n",
        "                          tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsBo0fGgBPUn"
      },
      "source": [
        "Using the batch approach, each gradient step requires us to make a prediction and compute the gradient for the whole data set, which makes each step take a long time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNqdgblbBPUn"
      },
      "source": [
        "Error functions are additive\n",
        "- The predictive error on the whole data set is simply the sum of the predictive errors for each data point.\n",
        "\n",
        "When this is the case, we can instead apply a technique called **stochastic gradient descent** \n",
        "- which computes the gradient (and takes a step) for only one point at a time. \n",
        "- It cycles over our data repeatedly until it reaches a stopping point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2XY_IcqBPUn"
      },
      "source": [
        "## Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz1V0VC8BPUo"
      },
      "source": [
        "During each cycle, we’ll want to iterate through our data in a random order:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-10-27T07:20:12.768921Z",
          "start_time": "2018-10-27T07:20:12.763245Z"
        },
        "id": "OFxDpV3kBPUo"
      },
      "outputs": [],
      "source": [
        "def in_random_order(data):\n",
        "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
        "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
        "    random.shuffle(indexes)                    # shuffle them\n",
        "    for i in indexes:                          # return the data in that order\n",
        "        yield data[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7XJgobtBPUp"
      },
      "source": [
        "This approach avoids circling around near a minimum forever\n",
        "- whenever we stop getting improvements we’ll decrease the step size and eventually quit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-10-28T07:35:19.050828Z",
          "start_time": "2018-10-28T07:35:19.015811Z"
        },
        "id": "X_0rm6K1BPUq"
      },
      "outputs": [],
      "source": [
        "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
        "    data = list(zip(x, y))\n",
        "    theta = theta_0                             # initial guess\n",
        "    alpha = alpha_0                             # initial step size\n",
        "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
        "    iterations_with_no_improvement = 0\n",
        "\n",
        "    # if we ever go 100 iterations with no improvement, stop\n",
        "    while iterations_with_no_improvement < 100:\n",
        "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
        "\n",
        "        if value < min_value:\n",
        "            # if we've found a new minimum, remember it\n",
        "            # and go back to the original step size\n",
        "            min_theta, min_value = theta, value\n",
        "            iterations_with_no_improvement = 0\n",
        "            alpha = alpha_0\n",
        "        else:\n",
        "            # otherwise we're not improving, so try shrinking the step size\n",
        "            iterations_with_no_improvement += 1\n",
        "            alpha *= 0.9\n",
        "\n",
        "        # and take a gradient step for each of the data points\n",
        "        for x_i, y_i in in_random_order(data):\n",
        "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
        "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
        "\n",
        "    return min_theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-10-28T07:35:20.053076Z",
          "start_time": "2018-10-28T07:35:20.048925Z"
        },
        "id": "HJD1O5ErBPUq"
      },
      "outputs": [],
      "source": [
        "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
        "    return minimize_stochastic(negate(target_fn),\n",
        "                               negate_all(gradient_fn),\n",
        "                               x, y, theta_0, alpha_0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-10-28T07:45:14.892154Z",
          "start_time": "2018-10-28T07:45:14.862917Z"
        },
        "id": "Fs-726B6BPUr"
      },
      "outputs": [],
      "source": [
        "print(\"using minimize_stochastic_batch\")\n",
        "\n",
        "x = list(range(101))\n",
        "y = [3*x_i + random.randint(-10, 20) for x_i in x]\n",
        "theta_0 = random.randint(-10,10) \n",
        "v = minimize_stochastic(sum_of_squares, sum_of_squares_gradient, x, y, theta_0)\n",
        "\n",
        "print(\"minimum v\", v)\n",
        "print(\"minimum value\", sum_of_squares(v))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNUFQUvrBPUs"
      },
      "source": [
        "Scikit-learn has a Stochastic Gradient Descent module  http://scikit-learn.org/stable/modules/sgd.html"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "幻灯片",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}