{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMPlWkLLb08RQB7D0A1BDr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MithunSR/Gradient_Descent_Tutorial/blob/main/Cost_Function_Binary_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "Binary classification tasks involve predicting one of two classes for a given instance. To evaluate and optimize models for such tasks, the cross-entropy loss function is widely used. This function measures the dissimilarity between predicted probabilities and true binary labels. By minimizing the cross-entropy loss, we aim to train a model that can accurately classify instances into one of the two classes.\n",
        "\n",
        "**Cross-Entropy Loss Function**\n",
        "\n",
        "The cross-entropy loss function, derived from information theory, is a popular choice due to its desirable properties. It penalizes confident and incorrect predictions more heavily, allowing the model to focus on correctly classifying challenging instances. The loss is defined as the average negative logarithm of the predicted probability for the correct class.\n",
        "\n",
        "**Dataset and Binary Classification**\n",
        "\n",
        "In a binary classification problem, we have a dataset comprising input features and corresponding binary labels, where each label represents one of the two classes (e.g., 0 or 1, negative or positive). Our goal is to build a model that can learn from the data and make accurate predictions on unseen instances.\n",
        "\n",
        "**Calculating Cross-Entropy Loss**\n",
        "\n",
        "To calculate the cross-entropy loss, we pass the input features through the model to obtain predicted probabilities. These probabilities are then compared to the true binary labels. If the true label is 1, the loss is the negative logarithm of the predicted probability for class 1. Conversely, if the true label is 0, the loss is the negative logarithm of the predicted probability for class 0. The average loss across all instances in the dataset gives us the overall cross-entropy loss.\n",
        "\n",
        "**Minimizing the Cross-Entropy Loss**\n",
        "\n",
        "Minimizing the cross-entropy loss is typically achieved using optimization algorithms like gradient descent. The model's parameters are iteratively adjusted in the direction that reduces the loss, improving the model's ability to accurately classify instances.\n",
        "\n",
        "**Application and Conclusion**\n",
        "\n",
        "By minimizing the cross-entropy loss, we aim to train a binary classification model that can make informed and accurate predictions on new and unseen instances. This has applications in various domains such as disease diagnosis, fraud detection, sentiment analysis, and more.\n",
        "\n",
        "Now that we have an understanding of the cross-entropy loss function and its role in binary classification, let's proceed to code examples that demonstrate its calculation and optimization."
      ],
      "metadata": {
        "id": "JZSh3u_2dubL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Example model"
      ],
      "metadata": {
        "id": "OIQF8EvagYKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Dependencies\n",
        "We begin by importing the necessary libraries for our code:\n",
        "\n",
        "numpy (as np) for numerical computations.\n",
        "matplotlib.pyplot (as plt) for plotting.\n",
        "We also import the load_breast_cancer function from sklearn.datasets and the train_test_split function from sklearn.model_selection."
      ],
      "metadata": {
        "id": "WaRiaS9GgeVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eCVvezJVc6Yw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Dependencies\n",
        "We begin by importing the necessary libraries for our code:\n",
        "\n",
        "numpy (as np) for numerical computations.\n",
        "matplotlib.pyplot (as plt) for plotting.\n",
        "We also import the load_breast_cancer function from sklearn.datasets and the train_test_split function from sklearn.model_selection."
      ],
      "metadata": {
        "id": "aI1Bk1Ffggmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Breast Cancer Wisconsin dataset\n",
        "data = load_breast_cancer()"
      ],
      "metadata": {
        "id": "HdMBIMvKeZI8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting the Dataset\n",
        "Next, we split the dataset into training and testing sets using the train_test_split function. We assign the feature matrices and target vectors for both the training and testing sets to X_train, X_test, y_train, and y_test, respectively."
      ],
      "metadata": {
        "id": "FvwWbIHHgrrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the features and target variable\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "JTM8nswbexr1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining the Sigmoid Function\n",
        "We define the sigmoid function, which maps any real-valued number to a value between 0 and 1. It is used to convert the linear function output into a probability."
      ],
      "metadata": {
        "id": "B3vkPd29guMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "kLZh6e8-e0yS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining the Hypothesis Function\n",
        "The hypothesis function computes the predicted probabilities using the sigmoid function. It takes the feature matrix X and the parameter vector theta as inputs and returns the predicted probabilities."
      ],
      "metadata": {
        "id": "GqBNAcRxgw47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hypothesis function\n",
        "def hypothesis(theta, X):\n",
        "    return sigmoid(np.dot(X, theta))"
      ],
      "metadata": {
        "id": "1rda9f7-e6du"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining the Cost Function\n",
        "The cost function calculates the cross-entropy loss, which measures the dissimilarity between the predicted probabilities and the true binary labels. It takes the parameter vector theta, the feature matrix X, and the target vector y as inputs. The cost is computed by comparing the predicted probabilities to the true labels and applying the cross-entropy formula."
      ],
      "metadata": {
        "id": "MVKTQn7vg4Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function (Cross-entropy loss)\n",
        "def cost_function(theta, X, y):\n",
        "    m = len(X)\n",
        "    h = hypothesis(theta, X)\n",
        "    error = y * np.log(h) + (1 - y) * np.log(1 - h)\n",
        "    cost = -np.sum(error) / m\n",
        "    return cost"
      ],
      "metadata": {
        "id": "Cff1f7CLe9RC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining the Gradient Descent Function\n",
        "The gradient descent function performs the optimization process by iteratively updating the parameter vector theta to minimize the cost function. It takes the initial parameter vector theta, the feature matrix X, the target vector y, the learning rate, and the number of iterations as inputs. In each iteration, it computes the gradients, updates the parameters, and calculates the cost. The updated parameters and the cost history are returned."
      ],
      "metadata": {
        "id": "PUo64K7zg57u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the gradient descent function\n",
        "def gradient_descent(theta, X, y, learning_rate, num_iterations):\n",
        "    m = len(X)\n",
        "    history = []\n",
        "    for _ in range(num_iterations):\n",
        "        h = hypothesis(theta, X)\n",
        "        error = h - y\n",
        "        gradient = np.dot(X.T, error) / m\n",
        "        theta -= learning_rate * gradient\n",
        "        cost = cost_function(theta, X, y)\n",
        "        history.append(cost)\n",
        "    return theta, history"
      ],
      "metadata": {
        "id": "xV28_VJ6fBuO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Adding a Bias Term\n",
        "We add a column of ones to the feature matrix X_train to account for the bias term in the linear equation. This allows the model to learn an intercept term."
      ],
      "metadata": {
        "id": "8AukkARhg-2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column of ones to the feature matrix for the bias term\n",
        "X_train_bias = np.c_[np.ones((len(X_train), 1)), X_train]"
      ],
      "metadata": {
        "id": "Wa7rVsWVfIfZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting Initial Values and Hyperparameters\n",
        "We set the initial values for the parameter vector theta, the learning rate, and the number of iterations."
      ],
      "metadata": {
        "id": "6f2q29G5hC-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the initial values and hyperparameters\n",
        "initial_theta = np.zeros(X_train_bias.shape[1])\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000"
      ],
      "metadata": {
        "id": "hScdkGZqfL0o"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Running Gradient Descent\n",
        "We call the gradient descent function with the initial values and hyperparameters to minimize the cost function. The optimized parameter vector theta and the cost history are returned."
      ],
      "metadata": {
        "id": "zTkDUf1rhH1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run gradient descent to minimize the cost function\n",
        "theta, cost_history = gradient_descent(initial_theta, X_train_bias, y_train, learning_rate, num_iterations)\n",
        "\n",
        "# Print the optimized values of theta\n",
        "print('Optimized theta:', theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4VWIPKofSFF",
        "outputId": "baba67f8-6bc8-4672-a5ac-d9290570ad1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized theta: [ 4.24516790e-01  3.24467767e+00  4.24208364e+00  1.88482765e+01\n",
            "  7.75655856e+00  2.92499148e-02 -1.48711100e-02 -5.91801434e-02\n",
            " -2.52974302e-02  5.50134051e-02  2.35598788e-02  1.27194627e-02\n",
            "  3.11289644e-01 -8.00874115e-02 -8.12217831e+00  1.56530348e-03\n",
            " -3.37237831e-03 -6.60996497e-03 -8.22805700e-04  4.97242660e-03\n",
            "  5.15483696e-04  3.40847459e+00  5.28081593e+00  1.90129635e+01\n",
            " -1.11976889e+01  3.57708025e-02 -6.52173044e-02 -1.27355091e-01\n",
            " -2.97116594e-02  7.05396872e-02  2.15588138e-02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-51a0d378df1b>:3: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-z))\n",
            "<ipython-input-6-4c16fc61af93>:5: RuntimeWarning: divide by zero encountered in log\n",
            "  error = y * np.log(h) + (1 - y) * np.log(1 - h)\n",
            "<ipython-input-6-4c16fc61af93>:5: RuntimeWarning: invalid value encountered in multiply\n",
            "  error = y * np.log(h) + (1 - y) * np.log(1 - h)\n"
          ]
        }
      ]
    }
  ]
}