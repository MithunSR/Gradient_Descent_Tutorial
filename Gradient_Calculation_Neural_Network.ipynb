{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOixZUWkr0lI4fvTLCuuwzu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MithunSR/Gradient_Descent_Tutorial/blob/main/Gradient_Calculation_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction:\n",
        "\n",
        "This code demonstrates how to compute the gradient of a neural network with multiple layers using backpropagation. It utilizes the Fashion MNIST dataset, a popular benchmark dataset for image classification tasks. The neural network model consists of an input layer, a hidden layer with ReLU activation, and an output layer with softmax activation. The model is trained using the Adam optimizer and sparse categorical cross-entropy loss."
      ],
      "metadata": {
        "id": "cXRlGQVRI8f4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the necessary libraries:\n",
        "\n",
        "numpy is imported as np for numerical computations.\n",
        "tensorflow is imported as tf for building and training neural networks.\n",
        "fashion_mnist module from tensorflow.keras.datasets is imported to access the Fashion MNIST dataset."
      ],
      "metadata": {
        "id": "8CLEl4GaJByF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "metadata": {
        "id": "ryFMSWXbJj-5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading and Preprocessing the data:\n",
        "\n",
        "The Fashion MNIST dataset is loaded using the fashion_mnist.load_data() function, and the train-test split is performed.\n",
        "The pixel values of the images are normalized by dividing them by 255.0 to bring them within the range of 0-1."
      ],
      "metadata": {
        "id": "Q8wdVH6cJG_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "NKekoKfVJlJ4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "faVVeEDSJwl0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining the neural network architecture:\n",
        "\n",
        "The neural network model is defined using the Sequential API from tensorflow.keras.\n",
        "The Flatten layer is used to convert the input images from 2D to 1D.\n",
        "The Dense layer with 128 units and ReLU activation is added as the hidden layer.\n",
        "The final Dense layer with 10 units and softmax activation is added as the output layer."
      ],
      "metadata": {
        "id": "oW5SQclYJIte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the architecture of the neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "cSh9EtJ2JrNN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compiling the model:\n",
        "\n",
        "The model is compiled with the Adam optimizer, which is a popular choice for gradient-based optimization.\n",
        "The sparse categorical cross-entropy loss function is used, suitable for multi-class classification tasks.\n",
        "The accuracy metric is also specified to monitor the performance of the model during training."
      ],
      "metadata": {
        "id": "XeQGZD-7JNaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "x96nbuH8J0Zy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the model:\n",
        "\n",
        "The model is trained using the fit function, which takes the training data, labels, number of epochs, batch size, and validation data.\n",
        "The training process iterates for the specified number of epochs, updating the model's parameters to minimize the loss."
      ],
      "metadata": {
        "id": "CQL5_H3eJQZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMSnq6QuJ6XH",
        "outputId": "8d04d45c-7473-49a3-e4c9-d2d629a68221"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5028 - accuracy: 0.8209 - val_loss: 0.4361 - val_accuracy: 0.8432\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3754 - accuracy: 0.8643 - val_loss: 0.3738 - val_accuracy: 0.8685\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3343 - accuracy: 0.8772 - val_loss: 0.3637 - val_accuracy: 0.8675\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3110 - accuracy: 0.8863 - val_loss: 0.3726 - val_accuracy: 0.8651\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2939 - accuracy: 0.8908 - val_loss: 0.3511 - val_accuracy: 0.8750\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2779 - accuracy: 0.8971 - val_loss: 0.3348 - val_accuracy: 0.8792\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2662 - accuracy: 0.9010 - val_loss: 0.3456 - val_accuracy: 0.8791\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2548 - accuracy: 0.9062 - val_loss: 0.3462 - val_accuracy: 0.8792\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2472 - accuracy: 0.9073 - val_loss: 0.3471 - val_accuracy: 0.8762\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2373 - accuracy: 0.9109 - val_loss: 0.3309 - val_accuracy: 0.8837\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f32c635bfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computing the gradient using backpropagation:\n",
        "\n",
        "The tf.GradientTape context is used to trace operations for automatic differentiation.\n",
        "The model's predictions are computed using the training data within the tape context.\n",
        "The compiled loss function is called with the true labels and predictions to calculate the loss.\n",
        "The gradients of the loss with respect to the trainable variables (model weights) are computed using tape.gradient."
      ],
      "metadata": {
        "id": "uwXIya08JTy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the gradient using backpropagation\n",
        "with tf.GradientTape() as tape:\n",
        "    predictions = model(X_train)\n",
        "    loss = model.compiled_loss(tf.squeeze(y_train), predictions)\n",
        "\n",
        "\n",
        "gradients = tape.gradient(loss, model.trainable_variables)"
      ],
      "metadata": {
        "id": "9YNCB8l3KCr0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Printing the gradients:\n",
        "\n",
        "Finally, the code loops through the model's trainable variables and corresponding gradients, printing their names and shapes."
      ],
      "metadata": {
        "id": "Bv9qdgsKJWi9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoGC0IVPHWv3",
        "outputId": "56947c01-81c3-4eab-a458-f784c8d2eee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dense_4/kernel:0 (784, 128)\n",
            "dense_4/bias:0 (128,)\n",
            "dense_5/kernel:0 (128, 10)\n",
            "dense_5/bias:0 (10,)\n"
          ]
        }
      ],
      "source": [
        "# Print the gradients for each layer\n",
        "for layer, gradient in zip(model.trainable_variables, gradients):\n",
        "    print(layer.name, gradient.shape)\n"
      ]
    }
  ]
}